{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#한겨레\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import re\n",
    "import urllib.parse\n",
    "from datetime import timedelta\n",
    "import datetime, re\n",
    "\n",
    "def hani_newsurl(keyword, date1, date2, pagenum):\n",
    "    url=(\"http://search.hani.co.kr/Search?command=query&keyword={0}\" \\\n",
    "\"&media=news&sort=d&period=all&datefrom={1}\" \\\n",
    "\"&dateto={2}\" \\\n",
    "\"&pageseq={3}\"\n",
    "    .format(keyword,date1,date2,pagenum))\n",
    "    resp = requests.get(url)\n",
    "    resp.raise_for_status()\n",
    "    resp.encoding = 'utf-8'\n",
    "    html = resp.text\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    my_titles = str(soup.select('dt,a'))\n",
    "    news_url = re.findall('href=\"(http:.+?)\"', my_titles)\n",
    "    return (news_url)\n",
    "\n",
    "date1=datetime.date(2018, 10, 1)\n",
    "date2=datetime.date(2018, 12, 31)\n",
    "keyword='유치원3법'\n",
    "\n",
    "url_list = set()\n",
    "while date1 < date2:\n",
    "    print(date1)\n",
    "    for num in range(20):\n",
    "        t = hani_newsurl(keyword, re.sub('-','.',str(date1)), re.sub('-','.',str(date2)), num)\n",
    "        url = [i for i in t if re.search('hani',i)]\n",
    "        for i in url:\n",
    "            url_list.add(i)\n",
    "        date1+=timedelta(days=+1)\n",
    "        \n",
    "def hani_news(url):\n",
    "    source_code_from_url = urllib.request.urlopen(url)\n",
    "    soup = BeautifulSoup(source_code_from_url, 'lxml', from_encoding = 'utf-8')\n",
    "    text = str(soup.select('#a-left-scroll-in > div.article-text > div > div.text'))\n",
    "    burn = re.compile('\\t|\\n|\\r|\\'|\\[|\\]|\\{.+|<.+?>')\n",
    "    article = burn.sub('',text)\n",
    "    article = re.sub('\\.','\\n',article)\n",
    "    press = 'hani'\n",
    "    date = str(soup.select('#article_view_headline > p.date-time'))\n",
    "    return (article, press, date) \n",
    "\n",
    "import pandas as pd \n",
    "\n",
    "article = []\n",
    "press = []\n",
    "date = []\n",
    "url = []\n",
    "\n",
    "for i in url_list:\n",
    "    if hani_news(i)[0]!='':\n",
    "        article.append(hani_news(i)[0])\n",
    "        press.append(hani_news(i)[1])\n",
    "        date.append(hani_news(i)[2])\n",
    "        url.append(i)   \n",
    "        \n",
    "new_date=[]\n",
    "for i in date:\n",
    "    if re.search('\\[<p class=\"date-time\"><span><em>등록 :</em>',i):\n",
    "        p = re.compile('\\[<p class=\"date-time\"><span><em>등록 :</em>(.+?)\\s')\n",
    "        new_date+=p.findall(i)\n",
    "    else:\n",
    "        new_date+=i\n",
    "date = [re.sub('-','.',i) for i in new_date]\n",
    "        \n",
    "data = {'article': article, 'press': press, 'date': date, 'url': url}\n",
    "df = pd.DataFrame(data, columns = ['article', 'press', 'date','url'])\n",
    "df.to_csv(r'C:/Users/chsjk/OneDrive/Documents/학업/4학년 겨울학기/웹스크롤러, 형태소 분석기,코퍼스/article/한겨레_유치원3법.csv')\n",
    "\n",
    "df[['article', 'press', 'date','url']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
